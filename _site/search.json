[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Robert Tulmen",
    "section": "",
    "text": "GIS Masters Student (3rd Semester)\nLocal Drone Enthusiast\nExplorer/Traveler"
  },
  {
    "objectID": "about.html#welcome-to-my-page",
    "href": "about.html#welcome-to-my-page",
    "title": "Robert Tulmen",
    "section": "Welcome to my page!",
    "text": "Welcome to my page!\nI’m currently working part-time for UTD as a student worker for the OIT department, I use my data analytics skills gained from my studies to produce visualizations for our social media sites to drive funding decisions and I also do part-time property management for a building in Highland Park - alongside my masters education here at UTD of course. I love technology, watching movies, and I live with two cats named Archimedes and Scout."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "External Drone Auditing",
    "section": "",
    "text": "I wanted to combine what I’d learned with the power of the DJI Mini 3 Pro. Although my drone has no imaging capabilities (other than stunning 4K video at 30 frames a second…) in combination with owning an Apple phone, meant I had to do everything myself manually. I had to designate my own flight path, tie points, and determine how many photos were enough to get an accurate 3D model to assess exterior damage… which I called an external audit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause of the rising costs in our everyday lives given Trump’s tariff era, I decided to focus my attention to homeowners in the DFW area. As you already might know my parents own this property in Highland Park, what you might not already know is that this area sees a lot of draft coming from the alley placed behind it. Given this information we receive a lot of wind damage to our composition shingles (more known as asphalt shingles). Roofing repairs used to cost us around 500 dollars a visit from our previous installer but given the rising rates due to tariffs (I know this because in the email we get from our roofer there is a disclaimer at the bottom to let you know that tariffs might affect pricing. We received a quote from the same employer for around 1800 making that a 260% increase from 500. Installing a new roof isn’t an option for us as we have new tenants moving in soon and we don’t possess the temporal equity to be able to move forward with that big an endeavor.\n\n\n\n\n\n\n\n\n\nThese shingles contain toxic metals such as Manganese, Copper, Zinc, Lead, and other harmful chemicals not to mention the petroleum and oil that goes into making these items for commercial usage. Another issue with this method of roofing is the breakdown of the chemical links between the products that make up these shingles from UV rays (the sun). As they breakdown they start to loosen and lose their integrity with the nails and eventually fall off. In tandem with rainwater leaking into the roof, you also see the PH levels rise in the coming days where rainfall is observed (Check out this study: Roofing as a source of non point water pollution – Science Direct). As a result, there is bio accumulation of lead and copper which are both toxic to the environment. And in particular, “However, mean Mn2+ concentrations in runoff from wood and composition shingle roofs were found to be significantly greater than in runoff from painted aluminum or galvanized roofs. (Which at higher quantities can be toxic to the environment)”. According to this roofing website: The Most Popular Roofing Materials in DFW | Dallas Roofing Experts, asphalt shingles are some of the most common roofing methods in the DFW metropolex which renders a concerning idea that we are polluting the environment whether we like it or not.\n\n\n\n\n\n\n\n\n\nMy goal with this project is to see if I can generate 3D models simply using a camera drone that isn’t necessarily built for jobs of this nature. With the growing interest in UASs I believe soon they will be akin to phones; in that they will be a necessity to keep up with modern technology. According to this site, Drone Market Size 2020-2025 | Drone Industry Insights written by an aerospace engineer, the drone market has increased by nearly 23 billion dollars since 2020 and is only going to go higher. With this significant growth we are going to see drones become more and more a part of our daily lives, including this masterful piece of machinery: Facility Inspection - Oil and Gas - Inspection - DJI Enterprise. Like mailboxes, what if docking stations were a part of standard life? Cutting out the time it takes for inspectors to get out in the field, getting accurate estimates of property YOU own, creating visualizations with a homeowners app in an instant. I believe this kind of change is imminent, which would allow us to take responsibility of our part in environmental sustainability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI tried to replicate that flight path of the automated app we used in class, but I found for that specific building it wouldn’t quite work with such an intimate area. So, I created an optimal flight path to adjust to my situation. The red dots you see on the right are uncalibrated images that were excluded from the 3D model building process due to being error prone from the angle being too flat and me breaking the homogenous overlap that I had originally created.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypically speaking with rendering 3D models, you want your RMSE to be as low as possible concerning the distribution of your tie points. Tie points are common visual features like corners, edges, or texture features that standout to Drone2Map that it can recognize and make a tie point at. The higher number of tie points you have the better your image is going to look, but it comes with the drawback of more storage, more data, and more time till completion. After the tie points are created you get solution points which are essentially those tie points bundled and adjusted (adjusted meaning camera orientation, angle, position, etc.…). My processing time originally was a lot longer after I got errors in my bottom few pictures, so instead of rendering the model with all 176 image I cut out about 10 to relieve those errors and complete the run time. The graph on the right is essentially a cross-match map that displays similar information to the graph on the left, showing tie points and their intersections.\n\n\n\n\n\n\n\n\n\nI thought that this was a fabulous looking model and a great output for what this project is trying to accomplish. Now that we live in this digital world, receiving your mail by car and mailbox is outdated anyway, correct? Imagine converting that wasted space into docking stations for all houses in neighborhood to get accurate assessments of structural damage. If that gets too expensive, imagine each neighborhood having its own designated drone to fly around and collect all sorts of vegetation health data, roof structure, wall integrity reports, and any other information under the sun that can be gathered by UAS to save the environment. Battery operated machinery that is cost effective in the long run and can accomplish a plethora of tasks to save us from destroying the Earth sounds like a win to me.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI later discovered on my trip that archaeologists had used drones with imaging capabilities to assess damage over time done to Macchu Picchu. So I’m making an effort to go in the Remote Sensing direction with respect to the Vilcanota mountains. Although my final decision hasn’t been finalized, with the help of my 3D Data Capture / LiDAR class, I hope to be able to narrow my scope soon. More to be added as it develops…\nhttps://www.mdpi.com/2072-4292/16/20/3901"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This is a page reserved for assignment one."
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Podcast Review",
    "section": "",
    "text": "What we’re missing—enterprise data. The thing that will separate all of us using the same model will be the quality and quantity of data we can obtain. The key to enterprise data is decision-making; it allows you to affect your future or have some forecast of what is going to happen. We “need to be able to make predictions,” according to Jure Leskovec. The AI revolution has taken us by force because of RCNNs. The way we process knowledge has completely diverged from the old ways, but decision-making is still archaic. ML and predictive AI have yet to be “touched,” and we can still make large advances towards that goal of highly accurate predictive modeling. Any kind of predictive tasks in tandem with loss functions can be tuned to any situation because of the non-factor of needing pre-training. Risk assessment, customer service, healthcare, finance—anything that gets better with predictive analysis—can be heavily influenced by graph-transformed models. There are still some nuances to the science, but the end goal, or “what is promised,” is no pre-training requirement and an accurate prediction in under a second. Right now, pre-trained models can be as accurate as senior data scientists, but if you want to push it further you can fine-tune the model by using data specific to your task and it becomes superhumanly accurate. This is especially great for fraud protection. Real-world examples would be user ad matching and ad relevance. Reddit, in a matter of a couple of months, built a “three-year” model by virtue of having so much data and a growing community to all pitch into it. The role of a data scientist won’t go away but will shift to modeling the data and determining the business impact. It will, however, be much faster and far more accurate, creating a new environment for several different disciplines."
  },
  {
    "objectID": "purpose.html",
    "href": "purpose.html",
    "title": "Purpose",
    "section": "",
    "text": "If you’re like me, you were born in the United States, but don’t quite feel like you’re at home.\nEvery chance I get I make an attempt to travel to places I’ve never seen before. Whether its backpacking through Europe with a buddy of mine, visiting my parents in Portugal, or going North to Canada to stop by for Christmas with my Oma, I’m always looking for a new adventure. Recently, after completing my second semester at UTD I had the idea to visit my study area for one of my final projects for my advanced GIS class. I booked a flight and found myself roaming the streets of Cusco Peru and had started my two week solo adventure in South America. My goal was to experience a different culture as well as try and grab relevant data with my DJI Mini 3 Pro for potential thesis data (Peru have very strict drone laws…), but ultimately I was there to feel small again. After visiting many different cities and historical sites I finally found myself at the biggest attraction, Macchu Picchu. At only 24 I had reached one of the World’s wonders and had the chance to explore this ancient relic of time, and also came to the conclusion that there’s so much more to see in this World than just Texas. I want to use the knowledge I’m accumulating from this masters and put it into practice all over the world…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robert Tulmen",
    "section": "",
    "text": "GIS Masters Student (3rd Semester)\nLocal Drone Enthusiast\nExplorer/Traveler"
  },
  {
    "objectID": "index.html#welcome-to-my-page",
    "href": "index.html#welcome-to-my-page",
    "title": "Robert Tulmen",
    "section": "Welcome to my page!",
    "text": "Welcome to my page!\nI’m currently working part-time for UTD as a student worker for the OIT department, I use my data analytics skills gained from my studies to produce visualizations for our social media sites to drive funding decisions and I also do part-time property management for a building in Highland Park - alongside my masters education here at UTD of course. I love technology, watching movies, and I live with two cats named Archimedes and Scout."
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment3",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "assignment3.html#working-with-to-identify-differences-in-trends-for-the-2024-elections---gtrendsr",
    "href": "assignment3.html#working-with-to-identify-differences-in-trends-for-the-2024-elections---gtrendsr",
    "title": "Assignment3",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "techstudy.html",
    "href": "techstudy.html",
    "title": "Mapping -> Technoledgy",
    "section": "",
    "text": "Simple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -123.0241 ymin: 33.38779 xmax: -117.4133 ymax: 39.3165\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME               estimate_MedianHHI moe_MedianHHI                  geometry\n   &lt;chr&gt;                           &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Santa Clara Count…             159674          1962 (((-122.2027 37.36305, -…\n 2 San Mateo County,…             156000          3019 (((-122.5209 37.59418, -…\n 3 Marin County, Cal…             142785          4083 (((-122.4451 37.86142, -…\n 4 San Francisco Cou…             141446          2576 (((-122.3777 37.83046, -…\n 5 Alameda County, C…             126240          1364 (((-122.3423 37.80556, -…\n 6 Contra Costa Coun…             125727          1829 (((-122.4298 37.9654, -1…\n 7 Placer County, Ca…             114678          1891 (((-121.4844 38.75135, -…\n 8 Orange County, Ca…             113702           986 (((-118.1146 33.74461, -…\n 9 Alpine County, Ca…             110781         18601 (((-120.0724 38.70277, -…\n10 Santa Cruz County…             109266          2959 (((-122.3177 37.18695, -…\n\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4096 ymin: 32.61864 xmax: -114.4629 ymax: 42.00948\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME               estimate_MedianHHI moe_MedianHHI                  geometry\n   &lt;chr&gt;                           &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Trinity County, C…              53498          7852 (((-123.6224 40.9317, -1…\n 2 Siskiyou County, …              55499          2854 (((-123.7184 41.59797, -…\n 3 Imperial County, …              56393          2266 (((-116.1063 32.66473, -…\n 4 Modoc County, Cal…              56648          3735 (((-121.4572 41.94994, -…\n 5 Lake County, Cali…              58738          2308 (((-123.0942 39.09529, -…\n 6 Sierra County, Ca…              60000         17579 (((-121.0575 39.53999, -…\n 7 Humboldt County, …              61135          1997 (((-124.4086 40.4432, -1…\n 8 Tehama County, Ca…              61834          3283 (((-123.0654 40.28697, -…\n 9 Lassen County, Ca…              64395          5496 (((-121.3319 40.91336, -…\n10 Mendocino County,…              64688          2951 (((-124.0233 40.00128, -…"
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Steam charts is a cool place to gather data on which games are the most popular amongst the steam population, which is one of the biggest platforms used by gamers. This code uses the base url of steam charts with minor modifications to go through the first 10 pages using a loop with the XPath from the HTML code.\n\n\n# A tibble: 6 × 7\n   Rank Name      `Current Players` `Last 30 Days` `Peak Players` `Hours Played`\n  &lt;dbl&gt; &lt;chr&gt;                 &lt;int&gt; &lt;lgl&gt;                   &lt;int&gt;          &lt;int&gt;\n1     1 Counter-…           1274415 NA                    1571060      681345750\n2     2 Dota 2               737481 NA                     953869      427383392\n3     3 PUBG: BA…            578070 NA                     707773      205646798\n4     4 Delta Fo…            216434 NA                     246418       79347051\n5     5 Apex Leg…            148337 NA                     201139       62213902\n6     6 Wallpape…            139489 NA                     141058       57902830\n# ℹ 1 more variable: Page &lt;int&gt;\n\n\n[1] \"Total rows scraped: 250\"\n\n\n# A tibble: 250 × 6\n    Rank Name              `Current Players` `Peak Players` `Hours Played`  Page\n   &lt;dbl&gt; &lt;chr&gt;                         &lt;int&gt;          &lt;int&gt;          &lt;int&gt; &lt;int&gt;\n 1     1 Counter-Strike 2            1274415        1571060      681345750     1\n 2     2 Dota 2                       737481         953869      427383392     1\n 3     3 PUBG: BATTLEGROU…            578070         707773      205646798     1\n 4     4 Delta Force                  216434         246418       79347051     1\n 5     5 Apex Legends                 148337         201139       62213902     1\n 6     6 Wallpaper Engine             139489         141058       57902830     1\n 7     7 Hollow Knight: S…            123651         462134      104464739     1\n 8     8 Bongo Cat                    116947         136696       71393689     1\n 9     9 Monster Hunter W…            114034         127322       10842522     1\n10    10 NARAKA: BLADEPOI…            111478         140882       34813042     1\n# ℹ 240 more rows"
  },
  {
    "objectID": "govscraping.html",
    "href": "govscraping.html",
    "title": "Scraping Government Data",
    "section": "",
    "text": "Using the script provided with minor adjustments, downloading one to five documents at a time takes a minuscule amount of time. Whether you have pre-made CSV, JSON file makes the process a lot easier and more manageable especially if those files are specific and as detailed as they were for us making running this script fairly simple.\nHowever, running the script that downloads ALL documents from the https://www.govinfo.gov/ is not a feasible way of scraping data especially if those pdf files are going to retain their size with all of the unnecessary information remaining in those documents (unnecessary information being distinguished by the web scraper for whichever goal you’re trying to achieve). My start time today running the script was 2:31PM and my end time was 3:01PM. So in 30 minutes I managed to download around one thousand pdf files that I have still to go through and dictate necessary information from unnecessary information. If the goal of this script was to scrape every document on a website that meets a specific criteria then that goal has been achieved and the script has served its purpose. But… If you were trying to gather all information on a specific topic and you didn’t have access to these ready made CSV/JSON files, this is not a feasible way of sifting through all the documentation. I would prefer keyword searches using third party applications, or tweaking this script to gather a more manageable data set that is readable unlike the hundreds of pdf files that I now have to manually go through in order for the script to retain some sort of value.\n\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  596053 31.9    1359808 72.7   596053 31.9\nVcells 1083481  8.3    8388608 64.0  1083481  8.3\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\\\\\"\n## For Mac, \"/Users/YOURNAME/path/\"\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles3$pdfLink\npdf_govfiles_id &lt;- govfiles3$index\n\n# Directory to save the pdf's\n# Be sure to create a folder for storing the pdf's\nsave_dir &lt;- \"yourpath\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\n# \n\n## Try downloading one document\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:1 %&gt;%\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 2.567229 secs\n\n## Try five\n# start.time &lt;- Sys.time()\n# message(\"Starting downloads\")\n# results &lt;- 1:5 %&gt;% \n#   purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n# message(\"Finished downloads\")\n# end.time &lt;- Sys.time()\n# time.taken &lt;- end.time - start.time\n# time.taken\n# \n# # Print results\n# print(results)\n# \n# # ## Download all: Caution, this may take a while and lots of space\n# start.time &lt;- Sys.time()\n# message(\"Starting downloads\")\n# results &lt;- 1:length(pdf_govfiles_url) %&gt;%\n#   purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n# message(\"Finished downloads\")\n# end.time &lt;- Sys.time()\n# time.taken &lt;- end.time - start.time\n# time.taken\n# print(results)\n# print(start.time)\n# print(end.time)"
  }
]